{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "255f24a5",
   "metadata": {},
   "source": [
    "# Main "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef47039",
   "metadata": {},
   "source": [
    "## Check Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053ffa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers_functions.setup import *\n",
    "\n",
    "path_archive = \"archive\"\n",
    "\n",
    "path_train_val_list = \"archive/train_val_list_NIH.txt\"\n",
    "path_test_list = \"archive/test_list_NIH.txt\"\n",
    "\n",
    "path_all_data_csv = \"archive/Data_Entry_2017.csv\"\n",
    "\n",
    "path_folder_images = \"archive/images-224/images-224\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4b7765",
   "metadata": {},
   "source": [
    "### Check structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f2d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path = path_archive\n",
    "print(json.dumps(list_tree(path, max_depth=1), indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cf30a3",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d8a9b8",
   "metadata": {},
   "source": [
    "### Create class-label linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb8c216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers_functions.multi_hot import *\n",
    "\n",
    "class_label_str_to_idx, class_label_idx_to_str = create_class_mappings(path_all_data_csv)\n",
    "\n",
    "print(class_label_str_to_idx)\n",
    "print(class_label_idx_to_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab42f799",
   "metadata": {},
   "source": [
    "### Multi-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ed7c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume class_to_idx and idx_to_class are already created\n",
    "image_to_multihot = create_image_multihot_mapping_from_dicts(path_all_data_csv, class_label_str_to_idx)\n",
    "\n",
    "# Check the first image mapping\n",
    "first_image = list(image_to_multihot.keys())[0]\n",
    "print(first_image, image_to_multihot[first_image])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b13cb68",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "#### Make a Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ff31c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import ResNet18_Weights\n",
    "from torchvision import transforms\n",
    "\n",
    "weights = ResNet18_Weights.DEFAULT\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.02, 0.02), scale=(0.9, 1.1)),\n",
    "    # transforms.RandomHorizontalFlip(p=0.5),  # only if medically safe\n",
    "    preprocess  # <- apply the pretrained model's resize/ToTensor/Normalize as one step\n",
    "])\n",
    "\n",
    "val_transforms = preprocess  # deterministic preprocess for validation/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0231616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "class SubsetWithTransform(Subset):\n",
    "    \"\"\"\n",
    "    Subset that overrides the transform used for samples by calling into the\n",
    "    underlying dataset but temporarily swapping its transform.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, indices, transform=None):\n",
    "        super().__init__(dataset, indices)\n",
    "        self._transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # idx is an index into the subset; map to dataset index\n",
    "        dataset_idx = self.indices[idx]\n",
    "        # save original transform and swap in ours\n",
    "        orig_transform = getattr(self.dataset, \"transform\", None)\n",
    "        self.dataset.transform = self._transform\n",
    "        try:\n",
    "            item = self.dataset[dataset_idx]\n",
    "        finally:\n",
    "            # restore original transform to avoid side-effects\n",
    "            self.dataset.transform = orig_transform\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb9cc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "IMAGE_SIZE = 224  # change to your model input\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.02,0.02), scale=(0.9, 1.1)),\n",
    "    # transforms.RandomHorizontalFlip(p=0.5),  # only if safe for your data\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize for 3-channel images (ImageNet stats as example)\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd82710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "class SubsetWithTransform(Subset):\n",
    "    \"\"\"\n",
    "    Subset that overrides the transform used for samples by calling into the\n",
    "    underlying dataset but temporarily swapping its transform.\n",
    "    This is safe because we won't use train and val loaders concurrently on the same dataset object.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, indices, transform=None):\n",
    "        super().__init__(dataset, indices)\n",
    "        self._transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # idx is an index into the subset; map to dataset index\n",
    "        dataset_idx = self.indices[idx]\n",
    "        # save original transform and swap in ours\n",
    "        orig_transform = getattr(self.dataset, \"transform\", None)\n",
    "        self.dataset.transform = self._transform\n",
    "        try:\n",
    "            item = self.dataset[dataset_idx]\n",
    "        finally:\n",
    "            # restore original transform to avoid side-effects\n",
    "            self.dataset.transform = orig_transform\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6955273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MedicalImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for medical images with precomputed multi-hot labels.\n",
    "\n",
    "    Each sample returns:\n",
    "        - image tensor\n",
    "        - multi-hot label vector\n",
    "        - image file name\n",
    "    \"\"\"\n",
    "    def __init__(self, files_list_dir: str, img_dir: str, image_to_multihot: dict, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            files_list_dir (str): Path to a .txt file with each row being an image name.\n",
    "            img_dir (str): Directory containing the images.\n",
    "            image_to_multihot (dict): Precomputed dictionary mapping image name â†’ multi-hot vector.\n",
    "            transform (callable, optional): Optional transform to apply to images.\n",
    "        \"\"\"\n",
    "        # Load image names from txt\n",
    "        with open(files_list_dir, \"r\") as f:\n",
    "            self.image_names = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "        self.img_dir = img_dir\n",
    "        self.image_to_multihot = image_to_multihot\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure no infinite loop\n",
    "        if idx >= 2 * len(self):\n",
    "            raise FileNotFoundError(f\"No valid images.\")\n",
    "\n",
    "        img_name = self.image_names[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "\n",
    "        # Skip missing images\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: Image file '{img_path}' not found. Skipping to next index.\")\n",
    "            return self.__getitem__((idx + 1) % len(self))  # wrap around if at end\n",
    "\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            # Default conversion to tensor\n",
    "            image = torch.from_numpy(np.array(image)).permute(2,0,1).float() / 255.0\n",
    "\n",
    "        # Get multi-hot vector from precomputed dictionary\n",
    "        multi_hot = torch.from_numpy(\n",
    "            self.image_to_multihot.get(\n",
    "                img_name,\n",
    "                np.zeros(len(next(iter(self.image_to_multihot.values()))),\n",
    "                dtype=np.float32)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return image, multi_hot, img_name\n",
    "\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "dataset = MedicalImageDataset(path_train_val_list, path_folder_images, image_to_multihot)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Get the first batch\n",
    "imgs, multi_hot, names = next(iter(loader))\n",
    "\n",
    "print(\"Image batch shape:\", imgs.shape)\n",
    "print(\"Label batch shape:\", multi_hot.shape)\n",
    "print(\"First image name:\", names[0])\n",
    "print(\"First label vector:\", multi_hot[0])\n",
    "\n",
    "# Display the first image in the batch\n",
    "plt.imshow(imgs[0].permute(1, 2, 0))\n",
    "plt.title(f\"Image: {names[0]}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7b210c",
   "metadata": {},
   "source": [
    "#### Create CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ca3484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.models import *\n",
    "\n",
    "model_name = \"Base\"\n",
    "model = models(model_name, backbone_name='resnet18')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75b0d36",
   "metadata": {},
   "source": [
    "#### Train the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfcaf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.train import *\n",
    "from models.val import *\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, Subset\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Train Parameters\n",
    "epochs = 10\n",
    "val_every = 5\n",
    "batch_size = 16\n",
    "save = True\n",
    "save_every = 1        # save checkpoint every `save_every` epochs\n",
    "download_after_save = True   # set True in Colab to auto-download checkpoints to your local machine\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# move model to device before creating optimizer\n",
    "model = model.to(device)\n",
    "\n",
    "num_label_classes = next(iter(image_to_multihot.values())).shape[0]\n",
    "print(\"Label vector length =\", num_label_classes)\n",
    "\n",
    "# If the model has a .fc layer (ResNet-style)\n",
    "if hasattr(model, \"fc\") and isinstance(model.fc, nn.Linear):\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, num_label_classes).to(device)\n",
    "    print(\"Updated model.fc to output =\", num_label_classes)\n",
    "else:\n",
    "    print(\"WARNING: model has no .fc; show me model structure if this fails.\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Split the dataset into train and val\n",
    "val_ratio = 0.2\n",
    "total_size = len(dataset)\n",
    "val_size = int(total_size * val_ratio)\n",
    "train_size = total_size - val_size\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "\n",
    "#train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "#NEW ADDITION FOR TRANSFORMATIONS\n",
    "perm = torch.randperm(total_size).tolist()\n",
    "train_idx = perm[:train_size]\n",
    "val_idx   = perm[train_size:]\n",
    "\n",
    "# build subsets that apply different transforms\n",
    "train_dataset = SubsetWithTransform(dataset, train_idx, transform=train_transforms)\n",
    "val_dataset   = SubsetWithTransform(dataset, val_idx,   transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\", flush=True)\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\", flush=True)\n",
    "\n",
    "# helper: move optimizer state tensors to cpu (recursively)\n",
    "def optimizer_state_cpu(opt_state_dict):\n",
    "    new = {\"state\": {}, \"param_groups\": opt_state_dict.get(\"param_groups\", [])}\n",
    "    for k, v in opt_state_dict.get(\"state\", {}).items():\n",
    "        if isinstance(v, dict):\n",
    "            new_state = {}\n",
    "            for kk, vv in v.items():\n",
    "                if isinstance(vv, torch.Tensor):\n",
    "                    new_state[kk] = vv.cpu()\n",
    "                else:\n",
    "                    new_state[kk] = vv\n",
    "            new[\"state\"][k] = new_state\n",
    "        else:\n",
    "            # fallback\n",
    "            new[\"state\"][k] = v\n",
    "    return new\n",
    "\n",
    "# helper: save checkpoint CPU-friendly\n",
    "def save_checkpoint(model, optimizer, epoch, val_loss=None, out_dir=\"checkpoints\", model_name=\"model\", download=False):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    # Move model params to cpu for portability\n",
    "    model_state_cpu = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "    opt_state = optimizer.state_dict()\n",
    "    opt_state_cpu = optimizer_state_cpu(opt_state)\n",
    "\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model_state_cpu,\n",
    "        \"optimizer_state\": opt_state_cpu,\n",
    "        \"val_loss\": val_loss,\n",
    "    }\n",
    "    fname = os.path.join(out_dir, f\"{model_name}_epoch{epoch:03d}.pth\")\n",
    "    torch.save(checkpoint, fname)\n",
    "    print(f\"Saved checkpoint -> {fname}\", flush=True)\n",
    "\n",
    "    # Colab browser download option (ignored if not in colab)\n",
    "    if download:\n",
    "        try:\n",
    "            from google.colab import files\n",
    "            files.download(fname)\n",
    "        except Exception as e:\n",
    "            print(\"Auto-download failed (not in Colab?). Reason:\", e, flush=True)\n",
    "\n",
    "# Training + validation loop (1 epoch at a time so we can checkpoint each epoch)\n",
    "start_time = time.perf_counter()\n",
    "best_val = float(\"inf\")\n",
    "checkpoint_dir = f\"checkpoints/{model_name}\"\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # train one epoch: note train(...) should handle one epoch between start_epoch and end_epoch\n",
    "    train(\n",
    "        model_name,\n",
    "        model,\n",
    "        optimizer,\n",
    "        train_dataset,\n",
    "        start_epoch=epoch - 1,\n",
    "        end_epoch=epoch,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        save=False,            # disable internal saving if train() can save (we handle saving here)\n",
    "        save_every=999,        # just in case; keep train from saving its own files\n",
    "        checkpoint_dir=checkpoint_dir\n",
    "    )\n",
    "\n",
    "    # validate according to val_every\n",
    "    val_loss = None\n",
    "    if (epoch % val_every == 0) or (epoch == epochs):\n",
    "        val_loss = val(\n",
    "            model_name,\n",
    "            model,\n",
    "            val_dataset,\n",
    "            epoch=epoch,\n",
    "            batch_size=batch_size,\n",
    "            device=device\n",
    "        )\n",
    "        # val(...) should return loss; if it doesn't, it'll be None and we still save\n",
    "\n",
    "        # keep best\n",
    "        if val_loss is not None and val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            print(f\"New best val {best_val:.6f} at epoch {epoch}\", flush=True)\n",
    "            # Save a \"best\" copy\n",
    "            save_checkpoint(model, optimizer, epoch, val_loss=val_loss, out_dir=checkpoint_dir, model_name=model_name, download=download_after_save)\n",
    "            # Also write latest.pth\n",
    "            latest_path = os.path.join(checkpoint_dir, f\"{model_name}_latest.pth\")\n",
    "            torch.save({\"epoch\": epoch, \"model_state\": {k: v.detach().cpu() for k, v in model.state_dict().items()}}, latest_path)\n",
    "        else:\n",
    "            # no improvement: maybe still save according to policy below\n",
    "            pass\n",
    "\n",
    "    # save every `save_every` epochs (and also save final epoch)\n",
    "    if save and ((epoch % save_every == 0) or (epoch == epochs)):\n",
    "        save_checkpoint(model, optimizer, epoch, val_loss=val_loss, out_dir=checkpoint_dir, model_name=model_name, download=download_after_save)\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"{epochs} Epochs model {model_name} train/val time: {elapsed_time:.4f} seconds\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aab63a",
   "metadata": {},
   "source": [
    "#### Classifier Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb156ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset and DataLoader\n",
    "test_dataset = MedicalImageDataset(path_test_list, path_folder_images, image_to_multihot)\n",
    "\n",
    "test_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Get the first batch\n",
    "imgs, multi_hot, names = next(iter(test_loader))\n",
    "\n",
    "print(\"Image batch shape:\", imgs.shape)\n",
    "print(\"Label batch shape:\", multi_hot.shape)\n",
    "print(\"First image name:\", names[0])\n",
    "print(\"First label vector:\", multi_hot[0])\n",
    "\n",
    "# Display the first image in the batch\n",
    "plt.imshow(imgs[0].permute(1, 2, 0))\n",
    "plt.title(f\"Image: {names[0]}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ad1f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers_functions.metrics import *\n",
    "from models.pred import *\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "from io import StringIO\n",
    "import sys\n",
    "\n",
    "# TODO: DELETE ONCE READY TO FULLY UTILIZE\n",
    "# Take first 100 elements\n",
    "#test_subset = Subset(test_dataset, indices=list(range(100)))\n",
    "\n",
    "save_dir = f\"results/{model_name}\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#capture printed output\n",
    "buffer = StringIO()\n",
    "stdout_original = sys.stdout\n",
    "sys.stdout = buffer  # redirect prints into buffer\n",
    "\n",
    "df, cm = evaluate(\n",
    "    model_name=model_name, \n",
    "    model=model,\n",
    "    pred=pred, \n",
    "    dataset=test_dataset)\n",
    "\n",
    "print(json.dumps(df.to_dict(), indent=2))\n",
    "\n",
    "print(\"Confusion Matrix\\n\", cm)\n",
    "\n",
    "# Restore stdout\n",
    "sys.stdout = stdout_original\n",
    "\n",
    "# Print to console normally\n",
    "print(buffer.getvalue())\n",
    "\n",
    "# Save captured text to file \n",
    "log_path = os.path.join(save_dir, \"metrics_output.txt\")\n",
    "with open(log_path, \"w\") as f:\n",
    "    f.write(buffer.getvalue())\n",
    "\n",
    "print(f\"\\nSaved printed metrics to:\\n  {log_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb740d2",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294c88e9",
   "metadata": {},
   "source": [
    "## Threshold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f6e123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, average_precision_score, roc_auc_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# path to the checkpoint\n",
    "path_checkpoint = \"checkpoints/ASL/ASL_epoch005 - best.pth\"\n",
    "out_dir = f\"results/{model_name}\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    val_dataset  \n",
    "except NameError:\n",
    "    raise RuntimeError(\"val_dataset not found in notebook.\")\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Robust load (reuse your load function or quick loader)\n",
    "def load_checkpoint_to_model(model, ckpt_path, device):\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    if isinstance(ckpt, dict):\n",
    "        for key in (\"model_state_dict\", \"state_dict\", \"model\", \"net\"):\n",
    "            if key in ckpt:\n",
    "                state = ckpt[key]; break\n",
    "        else:\n",
    "            state = ckpt\n",
    "    else:\n",
    "        state = ckpt\n",
    "\n",
    "    new_state = {}\n",
    "    for k, v in state.items():\n",
    "        nk = k[len(\"module.\"):] if k.startswith(\"module.\") else k\n",
    "        new_state[nk] = v\n",
    "    model.load_state_dict(new_state, strict=False)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Robust extraction of logits from model output\n",
    "def extract_logits(output):\n",
    "    \"\"\"\n",
    "    Return a torch.Tensor of logits.\n",
    "    Handles:\n",
    "      - tensor -> return it\n",
    "      - tuple/list -> take first element (if tensor)\n",
    "      - dict -> try common keys ('logits','out','pred','prediction')\n",
    "    \"\"\"\n",
    "    # direct tensor\n",
    "    if torch.is_tensor(output):\n",
    "        return output\n",
    "    # tuple or list\n",
    "    if isinstance(output, (tuple, list)):\n",
    "        if len(output) == 0:\n",
    "            raise ValueError(\"Model returned empty tuple/list\")\n",
    "        return extract_logits(output[0])\n",
    "    # dict-like\n",
    "    if isinstance(output, dict):\n",
    "        for key in (\"logits\", \"out\", \"pred\", \"prediction\", \"score\"):\n",
    "            if key in output:\n",
    "                return extract_logits(output[key])\n",
    "        # otherwise try to find the first tensor value\n",
    "        for v in output.values():\n",
    "            if torch.is_tensor(v):\n",
    "                return v\n",
    "    raise TypeError(f\"Cannot extract logits from model output of type {type(output)}\")\n",
    "\n",
    "# Get probs and labels (tuple-safe)\n",
    "@torch.no_grad()\n",
    "def get_probs_and_labels(model, loader, device):\n",
    "    all_scores = []\n",
    "    all_targets = []\n",
    "    filenames = []\n",
    "    model.eval()\n",
    "    for imgs, targets, names in tqdm(loader, desc=\"Predicting on val\"):\n",
    "        imgs = imgs.to(device)\n",
    "        raw_out = model(imgs)\n",
    "        # extract logits robustly\n",
    "        logits = extract_logits(raw_out)\n",
    "        # ensure logits is on CPU, detached\n",
    "        probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "        all_scores.append(probs)\n",
    "        all_targets.append(targets.numpy())\n",
    "        filenames.extend(names)\n",
    "    all_scores = np.vstack(all_scores)\n",
    "    all_targets = np.vstack(all_targets)\n",
    "    return all_scores, all_targets, filenames\n",
    "\n",
    "# find best per-class thresholds (same as before)\n",
    "def find_best_thresholds_by_f1(y_true, y_score, steps=101):\n",
    "    thresholds = np.linspace(0.0, 1.0, steps)\n",
    "    C = y_true.shape[1]\n",
    "    best_thresh = np.zeros(C)\n",
    "    best_f1 = np.zeros(C)\n",
    "    for c in range(C):\n",
    "        truths = y_true[:, c]\n",
    "        scores = y_score[:, c]\n",
    "        if truths.sum() == 0:\n",
    "            best_thresh[c] = 0.5\n",
    "            best_f1[c] = 0.0\n",
    "            continue\n",
    "        bf = -1.0\n",
    "        bt = 0.5\n",
    "        for t in thresholds:\n",
    "            preds = (scores >= t).astype(int)\n",
    "            f = f1_score(truths, preds, zero_division=0)\n",
    "            if f > bf:\n",
    "                bf = f\n",
    "                bt = t\n",
    "        best_thresh[c] = bt\n",
    "        best_f1[c] = bf\n",
    "    return best_thresh, best_f1\n",
    "\n",
    "def eval_with_thresholds(y_true, y_score, thresholds):\n",
    "    if np.isscalar(thresholds):\n",
    "        preds = (y_score >= thresholds).astype(int)\n",
    "    else:\n",
    "        thresh = np.array(thresholds)[None, :]\n",
    "        preds = (y_score >= thresh).astype(int)\n",
    "    C = y_true.shape[1]\n",
    "    per_class_f1 = np.array([f1_score(y_true[:, c], preds[:, c], zero_division=0) for c in range(C)])\n",
    "    micro_f1 = f1_score(y_true.ravel(), preds.ravel(), zero_division=0)\n",
    "\n",
    "    ap = []\n",
    "    auroc = []\n",
    "    for c in range(C):\n",
    "        truths = y_true[:, c]\n",
    "        scores = y_score[:, c]\n",
    "        try:\n",
    "            ap.append(average_precision_score(truths, scores))\n",
    "        except Exception:\n",
    "            ap.append(np.nan)\n",
    "        try:\n",
    "            auroc.append(roc_auc_score(truths, scores))\n",
    "        except Exception:\n",
    "            auroc.append(np.nan)\n",
    "    return {\n",
    "        \"per_class_f1\": per_class_f1,\n",
    "        \"micro_f1\": micro_f1,\n",
    "        \"mean_ap\": np.nanmean(ap),\n",
    "        \"mean_auroc\": np.nanmean(auroc),\n",
    "        \"ap_per_class\": np.array(ap),\n",
    "        \"auroc_per_class\": np.array(auroc),\n",
    "        \"preds\": preds\n",
    "    }\n",
    "\n",
    "# ---- Run: load checkpoint, predict, tune thresholds ----\n",
    "print(\"Loading checkpoint into model...\")\n",
    "model = load_checkpoint_to_model(model, path_checkpoint, device)  # uses model already created in notebook\n",
    "\n",
    "print(\"Running predictions on validation set...\")\n",
    "y_score, y_true, filenames = get_probs_and_labels(model, val_loader, device)\n",
    "print(\"Shapes: scores\", y_score.shape, \"truths\", y_true.shape)\n",
    "\n",
    "# Save raw arrays\n",
    "np.save(os.path.join(out_dir, f\"val_scores_{model_name}.npy\"), y_score)\n",
    "np.save(os.path.join(out_dir, f\"val_truths_{model_name}.npy\"), y_true)\n",
    "\n",
    "# Per-class tuning\n",
    "best_th, best_f1s = find_best_thresholds_by_f1(y_true, y_score, steps=101)\n",
    "print(\"Best per-class thresholds:\", best_th)\n",
    "print(\"Best per-class F1s:\", best_f1s)\n",
    "\n",
    "# Global threshold (micro-F1)\n",
    "grid = np.linspace(0.0, 1.0, 101)\n",
    "best_micro = -1.0\n",
    "best_global_t = 0.5\n",
    "for t in grid:\n",
    "    res = eval_with_thresholds(y_true, y_score, t)\n",
    "    if res[\"micro_f1\"] > best_micro:\n",
    "        best_micro = res[\"micro_f1\"]\n",
    "        best_global_t = t\n",
    "print(f\"Best global threshold: {best_global_t:.2f} (micro-F1 = {best_micro:.4f})\")\n",
    "\n",
    "# Save results\n",
    "res_perclass = eval_with_thresholds(y_true, y_score, best_th)\n",
    "out = {\n",
    "    \"best_per_class_thresholds\": best_th.tolist(),\n",
    "    \"best_per_class_f1s\": best_f1s.tolist(),\n",
    "    \"best_global_threshold\": float(best_global_t),\n",
    "    \"best_global_micro_f1\": float(best_micro),\n",
    "    \"ap_per_class\": res_perclass[\"ap_per_class\"].tolist(),\n",
    "    \"auroc_per_class\": res_perclass[\"auroc_per_class\"].tolist()\n",
    "}\n",
    "with open(os.path.join(out_dir, f\"thresholds_{model_name}.json\"), \"w\") as fh:\n",
    "    json.dump(out, fh, indent=2)\n",
    "print(\"Saved thresholds & metrics to\", out_dir)\n",
    "\n",
    "# Save predicted binary labels (using per-class thresholds)\n",
    "preds_perclass = (y_score >= best_th[None, :]).astype(int)\n",
    "np.save(os.path.join(out_dir, f\"val_preds_perclass_{model_name}.npy\"), preds_perclass)\n",
    "\n",
    "# Print per-class thresholds\n",
    "for i, (t, f) in enumerate(zip(best_th, best_f1s)):\n",
    "    label = class_label_idx_to_str[i] if 'class_label_idx_to_str' in globals() else str(i)\n",
    "    print(f\"Class {i:2d} ({label}): thresh={t:.2f}, F1={f:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3025f033",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = f\"results/{model_name}\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# y_score: (N, C) predicted probabilities, y_true: (N, C) binary truths\n",
    "if 'y_score' not in globals() or 'y_true' not in globals():\n",
    "    # try loading from saved numpy files\n",
    "    score_path = os.path.join(out_dir, f\"val_scores_{model_name}.npy\")\n",
    "    truth_path = os.path.join(out_dir, f\"val_truths_{model_name}.npy\")\n",
    "    if os.path.exists(score_path) and os.path.exists(truth_path):\n",
    "        y_score = np.load(score_path)\n",
    "        y_true = np.load(truth_path)\n",
    "        print(f\"Loaded y_score ({y_score.shape}) and y_true ({y_true.shape}) from {out_dir}\")\n",
    "    else:\n",
    "        raise RuntimeError(\"y_score / y_true not found in memory or results/. Run the tuning cell first.\")\n",
    "\n",
    "# best_th: per-class thresholds\n",
    "if 'best_th' not in globals():\n",
    "    thr_path = os.path.join(out_dir, \"thresholds_ManifoldMixup.json\")\n",
    "    if os.path.exists(thr_path):\n",
    "        with open(thr_path, \"r\") as fh:\n",
    "            thr_data = json.load(fh)\n",
    "        best_th = np.array(thr_data.get(\"best_per_class_thresholds\", thr_data.get(\"best_per_class_thresholds\", [0.5]*y_true.shape[1])))\n",
    "        print(f\"Loaded thresholds from {thr_path}\")\n",
    "    else:\n",
    "        # fallback to 0.5 if nothing found\n",
    "        best_th = np.full(y_true.shape[1], 0.5)\n",
    "        print(\"No thresholds found on disk; using 0.5 for all classes.\")\n",
    "\n",
    "# Class labels\n",
    "if 'class_label_idx_to_str' in globals():\n",
    "    labels = [class_label_idx_to_str[i] for i in range(len(best_th))]\n",
    "else:\n",
    "    labels = [str(i) for i in range(len(best_th))]\n",
    "\n",
    "# Apply thresholds to get binary predictions\n",
    "preds = (y_score >= best_th[None, :]).astype(int)  # (N, C)\n",
    "\n",
    "# Compute TP, FP, FN, TN per class\n",
    "TP = np.sum((preds == 1) & (y_true == 1), axis=0)\n",
    "FP = np.sum((preds == 1) & (y_true == 0), axis=0)\n",
    "FN = np.sum((preds == 0) & (y_true == 1), axis=0)\n",
    "TN = np.sum((preds == 0) & (y_true == 0), axis=0)\n",
    "\n",
    "# Build DataFrame for display\n",
    "df = pd.DataFrame({\n",
    "    \"label_idx\": list(range(len(best_th))),\n",
    "    \"label\": labels,\n",
    "    \"threshold\": best_th,\n",
    "    \"TP\": TP,\n",
    "    \"FP\": FP,\n",
    "    \"FN\": FN,\n",
    "    \"TN\": TN\n",
    "})\n",
    "# Add derived metrics (precision/recall/f1) for quick sanity check\n",
    "# Use zero_division=0 semantics: if denom is zero, set metric to 0.\n",
    "prec = []\n",
    "rec = []\n",
    "f1 = []\n",
    "for i in range(len(best_th)):\n",
    "    tp, fp, fn = int(TP[i]), int(FP[i]), int(FN[i])\n",
    "    p = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    r = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    if (p + r) > 0:\n",
    "        f = 2 * p * r / (p + r)\n",
    "    else:\n",
    "        f = 0.0\n",
    "    prec.append(p)\n",
    "    rec.append(r)\n",
    "    f1.append(f)\n",
    "\n",
    "df[\"precision\"] = np.round(prec, 4)\n",
    "df[\"recall\"] = np.round(rec, 4)\n",
    "df[\"f1_from_confusion\"] = np.round(f1, 4)\n",
    "\n",
    "# Print nicely\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "print(\"\\nPer-class confusion matrix and derived metrics:\\n\")\n",
    "display(df)\n",
    "\n",
    "# Save results\n",
    "csv_path = os.path.join(out_dir, \"per_class_confusion_ASL.csv\")\n",
    "json_path = os.path.join(out_dir, \"per_class_confusion_ASL.json\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "df.to_json(json_path, orient=\"records\", indent=2)\n",
    "print(f\"\\nSaved per-class confusion to:\\n  {csv_path}\\n  {json_path}\")\n",
    "\n",
    "# Print totals and micro/macro metrics \n",
    "total_TP = int(TP.sum()); total_FP = int(FP.sum()); total_FN = int(FN.sum()); total_TN = int(TN.sum())\n",
    "micro_precision = total_TP / (total_TP + total_FP) if (total_TP + total_FP) > 0 else 0.0\n",
    "micro_recall = total_TP / (total_TP + total_FN) if (total_TP + total_FN) > 0 else 0.0\n",
    "micro_f1 = 2 * micro_precision * micro_recall / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0.0\n",
    "\n",
    "macro_precision = float(np.nanmean(df[\"precision\"].replace(0, np.nan).fillna(0)))  # careful with zeros\n",
    "macro_recall = float(np.nanmean(df[\"recall\"].replace(0, np.nan).fillna(0)))\n",
    "macro_f1 = float(np.nanmean(df[\"f1_from_confusion\"].replace(0, np.nan).fillna(0)))\n",
    "\n",
    "print(\"\\nAggregated totals:\")\n",
    "print(f\"  TP={total_TP}, FP={total_FP}, FN={total_FN}, TN={total_TN}\")\n",
    "print(f\"Micro precision={micro_precision:.4f}, micro recall={micro_recall:.4f}, micro F1={micro_f1:.4f}\")\n",
    "print(f\"Macro precision (mean of per-class) ~ {macro_precision:.4f}, macro recall ~ {macro_recall:.4f}, macro F1 ~ {macro_f1:.4f}\")\n",
    "\n",
    "#show top classes by FP/ FN\n",
    "print(\"\\nTop 5 classes by FP (descending):\")\n",
    "display(df.sort_values(\"FP\", ascending=False).head(5)[[\"label_idx\",\"label\",\"FP\",\"TP\",\"FN\"]])\n",
    "\n",
    "print(\"\\nTop 5 classes by FN (descending):\")\n",
    "display(df.sort_values(\"FN\", ascending=False).head(5)[[\"label_idx\",\"label\",\"FN\",\"TP\",\"FP\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a006d",
   "metadata": {},
   "source": [
    "## Run Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a47658",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_name = \"ManiFold_Mixup\"\n",
    "model = models(model_name, backbone_name='resnet18')\n",
    "\n",
    "checkpoint_path = \"checkpoints/ManiFold_Mixup/ManiFold_Mixup_epoch005-best.pth\"\n",
    "print(\"Loading checkpoint:\", checkpoint_path)\n",
    "\n",
    "# Load checkpoint safely\n",
    "ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "# Try standard keys used in your training code\n",
    "if \"model_state_dict\" in ckpt:\n",
    "    state = ckpt[\"model_state_dict\"]\n",
    "elif \"state_dict\" in ckpt:\n",
    "    state = ckpt[\"state_dict\"]\n",
    "elif \"model\" in ckpt:\n",
    "    state = ckpt[\"model\"]\n",
    "else:\n",
    "    state = ckpt\n",
    "\n",
    "new_state = {}\n",
    "for k, v in state.items():\n",
    "    if k.startswith(\"module.\"):\n",
    "        new_state[k.replace(\"module.\", \"\")] = v\n",
    "    else:\n",
    "        new_state[k] = v\n",
    "\n",
    "model.load_state_dict(new_state, strict=False)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be08363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.pred import *\n",
    "# Load thresholds tuned for checkpoint 5\n",
    "with open(\"results/ManiFold_Mixup/thresholds_ManifoldMixup.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "thresholds = np.array(data[\"best_per_class_thresholds\"], dtype=np.float32)\n",
    "print(\"Loaded thresholds for checkpoint 5:\", thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d36df18",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, labels, preds = pred(\n",
    "    \"ManiFold_Mixup\",\n",
    "    model,\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    device=\"cuda\",\n",
    "    thresholds=thresholds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ad5510",
   "metadata": {},
   "source": [
    "## Evaluate threshold-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d984ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_f1 = f1_score(labels.ravel(), preds.ravel(), zero_division=0)\n",
    "macro_f1 = f1_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "macro_precision = precision_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "macro_recall = recall_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(\"\\n=== GLOBAL METRICS (WITH TUNED THRESHOLDS) ===\")\n",
    "print(f\"Micro F1:   {micro_f1:.4f}\")\n",
    "print(f\"Macro F1:   {macro_f1:.4f}\")\n",
    "print(f\"Macro Prec: {macro_precision:.4f}\")\n",
    "print(f\"Macro Rec:  {macro_recall:.4f}\")\n",
    "\n",
    "# ---- Per-class confusion matrix ----\n",
    "TP = ((preds == 1) & (labels == 1)).sum(axis=0)\n",
    "FP = ((preds == 1) & (labels == 0)).sum(axis=0)\n",
    "FN = ((preds == 0) & (labels == 1)).sum(axis=0)\n",
    "TN = ((preds == 0) & (labels == 0)).sum(axis=0)\n",
    "\n",
    "F1_perclass = []\n",
    "AP_perclass = []\n",
    "AUROC_perclass = []\n",
    "\n",
    "c = labels.shape[1]\n",
    "\n",
    "for c in range(C):\n",
    "    F1_perclass.append(f1_score(labels[:, c], preds[:, c], zero_division=0))\n",
    "    try:\n",
    "        AP_perclass.append(average_precision_score(labels[:, c], probs[:, c]))\n",
    "    except:\n",
    "        AP_perclass.append(np.nan)\n",
    "    try:\n",
    "        AUROC_perclass.append(roc_auc_score(labels[:, c], probs[:, c]))\n",
    "    except:\n",
    "        AUROC_perclass.append(np.nan)\n",
    "\n",
    "df_eval = pd.DataFrame({\n",
    "    \"Class\": np.arange(C),\n",
    "    \"TP\": TP,\n",
    "    \"FP\": FP,\n",
    "    \"FN\": FN,\n",
    "    \"TN\": TN,\n",
    "    \"F1\": np.round(F1_perclass, 4),\n",
    "    \"AP\": np.round(AP_perclass, 4),\n",
    "    \"AUROC\": np.round(AUROC_perclass, 4),\n",
    "})\n",
    "\n",
    "print(\"\\n=== PER-CLASS METRICS ===\")\n",
    "display(df_eval)\n",
    "\n",
    "# Save output\n",
    "df_eval.to_csv(\"results/final_eval_with_thresholds.csv\", index=False)\n",
    "print(\"\\nSaved: results/final_eval_with_thresholds.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
